{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SITHAR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UopR1sVuUrQD",
        "outputId": "06ffea13-b60d-446d-a18b-801cef83a9cf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxsGJQuQVjxn",
        "outputId": "0aaf8669-87bd-4bf7-9feb-9b510cec2bd8"
      },
      "source": [
        "!pip install python-docx\n",
        "!nvidia-smi\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import docx\n",
        "import re\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Dropout, Bidirectional\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.7/dist-packages (0.8.11)\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from python-docx) (4.2.6)\n",
            "Wed May 19 11:53:22 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elL3dafB6PZl"
      },
      "source": [
        "class data_processing:\n",
        "  \n",
        "  def __init__(self,max_vocab , sequence_length):\n",
        "    self.sequence_length = sequence_length\n",
        "    self.max_vocab = max_vocab\n",
        "    self.tokenizer = Tokenizer(num_words = self.max_vocab)\n",
        "    self.text = self.getText(\"/content/drive/My Drive/sitharasongs.docx\")\n",
        "\n",
        "    self.lower_data = self.text.lower()           \n",
        "    self.split_data = self.lower_data.splitlines()      \n",
        "    self.final = ''     \n",
        "\n",
        "    for line in self.split_data:\n",
        "        line = self.clean_text(line)\n",
        "        self.final += '\\n' + line\n",
        "\n",
        "    self.final_data = self.final.split('\\n')       \n",
        "    self.tokenizer.fit_on_texts(self.final_data)\n",
        "    self.word2idx = self.tokenizer.word_index\n",
        "    self.idx2word = dict(map(reversed, self.word2idx.items()))\n",
        "    self.words = self.word2idx.keys()\n",
        "    self.words_indexes = [self.word2idx[w] for w in self.words]\n",
        "    self.len_word = len(self.words_indexes)\n",
        "\n",
        "  def data_xy(self,index):    \n",
        "      x = self.words_indexes[index : index + self.sequence_length]\n",
        "      y = self.words_indexes[index + 1 : index + self.sequence_length+1]\n",
        "      return {\"X\":x , \"Y\":y}\n",
        "   \n",
        "  def getText(self,filename):\n",
        "      doc = docx.Document(filename)\n",
        "      fullText = []\n",
        "      for para in doc.paragraphs:\n",
        "          fullText.append(para.text)\n",
        "      return '\\n'.join(fullText)\n",
        "\n",
        "  def clean_text(self,text):\n",
        "      text = re.sub(r',', '', text)\n",
        "      text = re.sub(r'\\'', '',  text)\n",
        "      text = re.sub(r'\\\"', '', text)\n",
        "      text = re.sub(r'\\(', '', text)\n",
        "      text = re.sub(r'\\)', '', text)\n",
        "      text = re.sub(r'\\n', '', text)\n",
        "      text = re.sub(r'“', '', text)\n",
        "      text = re.sub(r'”', '', text)\n",
        "      text = re.sub(r'’', '', text)\n",
        "      text = re.sub(r'\\.', '', text)\n",
        "      text = re.sub(r';', '', text)\n",
        "      text = re.sub(r':', '', text)\n",
        "      text = re.sub(r'\\-', '', text)\n",
        "      return text\n",
        "\n",
        "  def data_generator(self,batch_size = 10):\n",
        "    while True:\n",
        "      x_batch = np.empty((batch_size, self.sequence_length), dtype='int64') \n",
        "      y_batch = np.empty((batch_size, self.sequence_length), dtype='int64') \n",
        "      randomlist = random.sample(range(self.len_word-self.sequence_length), batch_size)\n",
        "      for i in randomlist:\n",
        "        data = self.data_xy(i)\n",
        "        x_batch[:] = data[\"X\"]\n",
        "        y_batch[:] = data[\"Y\"]\n",
        "      yield (x_batch , y_batch)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2FA9tOipdPP"
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  \n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super().__init__(self)\n",
        "    self.Embedding = Embedding(vocab_size, embedding_dim)\n",
        "    self.Dropout = Dropout(0.2)\n",
        "    self.Bidirectional = Bidirectional(layer=LSTM(340, return_sequences=True))\n",
        "    self.LSTM = LSTM(100 , return_sequences=True)\n",
        "    self.Dense_1 = Dense(1024, activation='relu')\n",
        "    self.Dense_2 = Dense(vocab_size, activation='softmax')\n",
        "\n",
        "  def call(self, inputs, return_state=False):\n",
        "    x = inputs\n",
        "    x = self.Embedding(x) \n",
        "    x = self.Bidirectional(x)\n",
        "    x = self.Dropout(x)\n",
        "    x = self.LSTM(x)\n",
        "    x = self.Dense_1(x)\n",
        "    x = self.Dropout(x)\n",
        "    x = self.Dense_2(x)\n",
        "    return x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jLEVvRzeQL0"
      },
      "source": [
        "class prediction:\n",
        "  def __init__(self, model,tokenizer, idx2words ,temperature=1.0):\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.tokenizer = tokenizer\n",
        "    self.idx2word  = idx2word\n",
        "\n",
        "  def pred_next_word(self, inputs):\n",
        "\n",
        "    input_ids = self.tokenizer.word_index[inputs]\n",
        "    input_ids = np.array([[input_ids]], dtype=np.float32)\n",
        "    predicted_logits = self.model(inputs=input_ids)\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "    predicted_chars = self.idx2word[int(np.squeeze(np.squeeze(predicted_ids)))]\n",
        "    \n",
        "    return predicted_chars"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJgVtw849h9N"
      },
      "source": [
        "def train_model(max_vocab , sequence_length):\n",
        "\n",
        "  df = data_processing(max_vocab=2101 , sequence_length = 20)\n",
        "  sequence_length = 20\n",
        "  EPOCHS = 5\n",
        "  vocab_size = df.len_word\n",
        "  model = MyModel(vocab_size,embedding_dim=256)\n",
        "  loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "  checkpoint_dir = '/content/drive/My Drive/training_checkpoints'\n",
        "  checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "  checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True)\n",
        "\n",
        "  model.compile(optimizer='adam', loss=loss)\n",
        "  history = model.fit_generator(df.data_generator(),epochs=EPOCHS,steps_per_epoch=1 ,callbacks=[checkpoint_callback])\n",
        "  return model , df.tokenizer,df.idx2word"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvrYibTKB-oy",
        "outputId": "199afe52-b7d4-4e89-9e24-4a17ee5ca645"
      },
      "source": [
        "if __name__==\"__main__\":\n",
        "\n",
        "  model_prediction ,tokenizer ,idx2word = train_model(max_vocab =2101 ,sequence_length = 256)\n",
        "  pred = prediction(model_prediction, tokenizer , idx2word)\n",
        "  next_char = input(\"enter the keyword\")\n",
        "  result = [next_char]\n",
        "  for n in range(20):\n",
        "    next_char = pred.pred_next_word(next_char)\n",
        "    result.append(next_char)\n",
        "    result.append(\" \")\n",
        "\n",
        "  result = tf.strings.join(result)\n",
        "  print(result.numpy().decode('utf-8'), '\\n\\n' + '_'*80)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1/1 [==============================] - 5s 5s/step - loss: 7.6497\n",
            "Epoch 2/5\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 7.6502\n",
            "Epoch 3/5\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 7.6506\n",
            "Epoch 4/5\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 7.6502\n",
            "Epoch 5/5\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 7.6515\n",
            "enter the keywordnjan\n",
            "njanenthoru pulkidumini manase tharaam viriyunnorishlormmakal eenangal ulakin kadalinte maunavum kondille kavaruthirunnundu choodumo kinnarathinum kudamoru ozhukiyoo… pulleem poy eeran nadhiyaay velayil  \n",
            "\n",
            "________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA9DejdxeZRY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}